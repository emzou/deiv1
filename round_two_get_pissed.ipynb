{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/easy/.pyenv/versions/3.11.5/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time \n",
    "import random\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import entropy, spearmanr\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.spatial.distance import jensenshannon, cosine\n",
    "import regex as re\n",
    "import spacy\n",
    "import string\n",
    "import os\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "import json\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "device_id = 0 if torch.backends.mps.is_available() else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = pd.read_csv(\"gecko_pelts.csv\", index_col = 0)\n",
    "energy_drink = [x for x in dm['word']]\n",
    "da = pd.read_csv(\"jan_mar_2023poldumps.csv\", index_col = 0)\n",
    "dg = da.dropna().drop_duplicates(subset=\"comment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name).to(device)\n",
    "fill_mask_pipeline = pipeline(\"fill-mask\", model=model_name, device=device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_prose(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r\"^>\\s*\", \"\", text)  # Remove leading '>'\n",
    "    text = re.sub(r\">>\\d+\\s*\", \"\", text)  # Remove quote references\n",
    "    text = re.sub(r\"^[^a-zA-Z]+|[^a-zA-Z.!?]+$\", \"\", text)  # Trim unwanted characters\n",
    "    text = re.sub(r\"\\n+\", \" \", text)  # Replace newlines with spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize whitespace\n",
    "    text = re.sub(r\"http[s]?://[^\\s>]+|www\\.[^\\s>]+\", \"\", text)  # Remove URLs\n",
    "    return text\n",
    "\n",
    "def split_sentences(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    return [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n",
    "\n",
    "def filter_by_word(df, column, word):\n",
    "    if not isinstance(word, str):\n",
    "        raise ValueError(f\"Invalid word input: expected string, got {type(word)}\")\n",
    "    \n",
    "    pattern = re.compile(rf'\\b{re.escape(word)}\\b', re.IGNORECASE)\n",
    "    return df[df[column].astype(str).str.contains(pattern, na=False)]\n",
    "\n",
    "def filter_strings_by_word(sentences, word):\n",
    "    if not isinstance(word, str):\n",
    "        raise ValueError(f\"Invalid word input: expected string, got {type(word)}\")\n",
    "    \n",
    "    pattern = re.compile(rf'\\b{re.escape(word)}\\b', re.IGNORECASE)\n",
    "    return [s for s in sentences if pattern.search(s)]\n",
    "\n",
    "def is_valid_word(word):\n",
    "    return bool(re.match(r\"^[a-zA-Z]+$\", word))\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    df.drop_duplicates(subset=\"comment\", keep=\"first\", inplace=True)\n",
    "    df = df[df[\"comment\"].apply(lambda x: len(str(x).split(\" \")) <= 512)]\n",
    "    return df.reset_index(drop=True) if len(df) >= 100 else pd.DataFrame()\n",
    "\n",
    "def og_thedf(df, word):\n",
    "    df[\"comment\"] = df[\"comment\"].apply(clean_prose)\n",
    "    df = filter_by_word(df, \"comment\", word)\n",
    "    df[\"com_list_it1\"] = df[\"comment\"].apply(split_sentences)\n",
    "    df[\"sent_with_word\"] = df[\"com_list_it1\"].apply(lambda x: filter_strings_by_word(x, word))\n",
    "    df = df.explode(\"sent_with_word\").drop_duplicates(subset=[\"comment\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def getopdf(df, word):\n",
    "    return og_thedf(df, word)[lambda x: x[\"op\"] == 1]\n",
    "\n",
    "def group_threads_by_timestamp(df, thread_col=\"thread_num\", num_col=\"num\", comment_col=\"comment\", timestamp_col=\"timestamp\"):\n",
    "    thread_dict = defaultdict(list)\n",
    "    for thread, thread_df in df.groupby(thread_col):\n",
    "        sorted_comments = thread_df.sort_values(by=timestamp_col)[[num_col, comment_col]].to_dict(orient='records')\n",
    "        thread_dict[thread] = sorted_comments\n",
    "    return thread_dict\n",
    "\n",
    "def df_to_thread_dict(df):\n",
    "    thread_dict = {\n",
    "        thread: thread_df.sort_values(by=\"timestamp\")[[\"num\", \"comment\"]].to_dict(orient='records')\n",
    "        for thread, thread_df in df.groupby(\"thread_num\")\n",
    "    }\n",
    "    return thread_dict\n",
    "\n",
    "def precompute_thread_dict(df):\n",
    "    thread_dict = {}\n",
    "    for thread, thread_df in df.groupby(\"thread_num\"):\n",
    "        thread_dict[thread] = thread_df.sort_values(by=\"timestamp\")[[\"num\", \"comment\"]].to_dict(orient='records')\n",
    "    return thread_dict\n",
    "\n",
    "def pretokenize_comments(thread_dict):\n",
    "    for thread, posts in thread_dict.items():\n",
    "        for post in posts:\n",
    "            post['tokenized_comment'] = set(post['comment'].lower().split())  # Store pre-tokenized words\n",
    "    return thread_dict\n",
    "\n",
    "karina = pretokenize_comments(precompute_thread_dict(dg)) ### TURNING DATAFRAME INTO A DICTIONARY OF THREADS!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_valid_threads(thread_dict, words):\n",
    "    valid_thread_counts = {}\n",
    "    for word in words:\n",
    "        if not isinstance(word, str):\n",
    "            print(f\"Skipping invalid word: {word} (not a string)\")\n",
    "            continue\n",
    "        word_pattern = re.compile(rf'\\b{re.escape(word)}\\b', re.IGNORECASE)\n",
    "        valid_threads = 0\n",
    "\n",
    "        for thread_id, comments in thread_dict.items():\n",
    "            # Find all comments containing the target word\n",
    "            appearances = (c for c in comments if word_pattern.search(c['comment']))\n",
    "            appearances_list = list(appearances)\n",
    "\n",
    "            # Count only threads where the word appears at least twice\n",
    "            if len(appearances_list) > 1:\n",
    "                valid_threads += 1\n",
    "\n",
    "        if valid_threads >= 100:  # Only keep words that have at least 100 valid threads\n",
    "            valid_thread_counts[word] = valid_threads\n",
    "    return valid_thread_counts\n",
    "\n",
    "def filter_and_extract_word_stats(thread_dict, target_word):\n",
    "    if not isinstance(target_word, str):\n",
    "        raise ValueError(f\"Invalid target_word input: expected string, got {type(target_word)}\")\n",
    "    word_pattern = re.compile(rf'\\b{re.escape(target_word)}\\b', re.IGNORECASE)\n",
    "    data = []\n",
    "    for thread_id, comments in thread_dict.items():\n",
    "        # Use a generator to find comments containing the target word (faster than list comprehensions)\n",
    "        appearances = (c for c in comments if word_pattern.search(c['comment']))\n",
    "        appearances_list = list(appearances)\n",
    "        if len(appearances_list) > 1:\n",
    "            first_comment, last_comment = appearances_list[0], appearances_list[-1]\n",
    "\n",
    "            data.append({\n",
    "                'thread_id': thread_id,\n",
    "                'first_appearance': first_comment['comment'],\n",
    "                'first_id': first_comment['num'],\n",
    "                'last_appearance': last_comment['comment'],\n",
    "                'last_id': last_comment['num'],\n",
    "                'length': len(comments)  # Total number of comments in thread\n",
    "            })\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_df(a_df, a_word, tokenizer = tokenizer):\n",
    "    if not isinstance(a_word, str):\n",
    "        raise ValueError(f\"Invalid a_word input: expected string, got {type(a_word)}\")\n",
    "    mask_token = tokenizer.mask_token\n",
    "    word_pattern = re.compile(rf'\\b{re.escape(a_word)}\\b', re.IGNORECASE)\n",
    "    \n",
    "    def masker(text):\n",
    "        return word_pattern.sub(mask_token, text) if isinstance(text, str) else text\n",
    "    \n",
    "    def trim_long_text(text, limit=450):\n",
    "        if not isinstance(text, str):\n",
    "            return text\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        if len(tokens) <= limit:\n",
    "            return text\n",
    "        words = text.split()\n",
    "        word_indices = [i for i, w in enumerate(words) if word_pattern.search(w)]\n",
    "        if not word_indices:\n",
    "            return \" \".join(words[:limit])\n",
    "        \n",
    "        target_index = word_indices[0]\n",
    "        left, right = max(0, target_index - limit//2), min(len(words), target_index + limit//2)\n",
    "        return \" \".join(words[left:right])\n",
    "    \n",
    "    a_df[\"first_appearance\"] = a_df[\"first_appearance\"].apply(masker)\n",
    "    a_df[\"last_appearance\"] = a_df[\"last_appearance\"].apply(masker)\n",
    "    a_df = a_df[a_df[\"first_appearance\"].str.contains(mask_token, na=False) | a_df[\"last_appearance\"].str.contains(mask_token, na=False)]\n",
    "    a_df[\"first_appearance\"] = a_df[\"first_appearance\"].apply(trim_long_text)\n",
    "    a_df[\"last_appearance\"] = a_df[\"last_appearance\"].apply(trim_long_text)\n",
    "    return a_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_samples(df, trial_num):\n",
    "    dfs = []\n",
    "    for i in range(1, trial_num + 1):\n",
    "        dfs.append(df.sample(n=20, random_state=i))\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_meaning_unrecognized(df, mask_column, target_word, fill_mask_pipeline=fill_mask_pipeline, tokenizer=tokenizer, top_k=10):\n",
    "    predictions_list = []\n",
    "    mask_token = tokenizer.mask_token\n",
    "    word_pattern = re.compile(rf'\\b{re.escape(target_word)}\\b', re.IGNORECASE)\n",
    "    punctuation_set = set(string.punctuation)\n",
    "    df[mask_column] = df[mask_column].astype(str)\n",
    "    sentences = df[mask_column].str.strip().tolist()\n",
    "    processed_sentences = [word_pattern.sub(mask_token, sentence.lower()) for sentence in sentences if sentence]\n",
    "    # Skip sentences longer than 512 tokens and print a warning\n",
    "    filtered_sentences = []\n",
    "    for sentence in processed_sentences:\n",
    "        if len(tokenizer.tokenize(sentence)) > 512:\n",
    "            print(f\"Skipping sentence (exceeds 512 tokens): {sentence[:100]}...\")\n",
    "            continue\n",
    "        filtered_sentences.append(sentence)\n",
    "    \n",
    "    if not filtered_sentences:\n",
    "        return predictions_list  # Return empty if no valid sentences remain\n",
    "\n",
    "    # Batch tokenization with GPU support\n",
    "    tokenized_sentences = tokenizer.batch_encode_plus(filtered_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    tokenized_sentences = {k: v.to(device) for k, v in tokenized_sentences.items()}  # Move to GPU if available\n",
    "\n",
    "    # **Batch Inference Optimization with torch.no_grad()**\n",
    "    with torch.no_grad():\n",
    "        predictions_batch = fill_mask_pipeline(filtered_sentences)  # **Send all at once instead of looping**\n",
    "    \n",
    "    for predictions in predictions_batch:\n",
    "        if not isinstance(predictions, list) or 'token_str' not in predictions[0]:\n",
    "            continue\n",
    "\n",
    "        filtered_predictions = {\n",
    "            p['token_str'].strip(): np.log(p['score'])\n",
    "            for p in predictions\n",
    "            if 'token_str' in p and not all(char in punctuation_set for char in p['token_str'])\n",
    "        }\n",
    "\n",
    "        if not filtered_predictions:\n",
    "            continue\n",
    "        log_probs = np.array(list(filtered_predictions.values()))\n",
    "        max_log_prob = np.max(log_probs)\n",
    "        exp_probs = np.exp(log_probs - max_log_prob)\n",
    "        normalized_probs = dict(zip(filtered_predictions.keys(), exp_probs / exp_probs.sum()))\n",
    "        predictions_list.append(normalized_probs)\n",
    "    return predictions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(word):\n",
    "    doc = nlp(word)\n",
    "    return doc[0].pos_ if doc and doc[0].pos_ else \"UNKNOWN\"\n",
    "\n",
    "def get_first_guess(dict_list):\n",
    "    guess_counts = Counter()\n",
    "    for d in dict_list:\n",
    "        if d:\n",
    "            best_guess = max(d, key=d.get)  # Get the word with the highest probability\n",
    "            guess_counts[best_guess] += 1\n",
    "    return guess_counts.most_common(1)[0][0] if guess_counts else None\n",
    "\n",
    "def cosine_similarity(p, q):\n",
    "    all_keys = set(p.keys()).union(set(q.keys()))\n",
    "    p_vec = np.array([p.get(k, 0) for k in all_keys])\n",
    "    q_vec = np.array([q.get(k, 0) for k in all_keys])\n",
    "    \n",
    "    if np.all(p_vec == 0) or np.all(q_vec == 0):  # Avoid division by zero\n",
    "        return 0.0\n",
    "    return 1 - cosine(p_vec, q_vec)\n",
    "\n",
    "def wasserstein_similarity(p, q):\n",
    "    all_keys = list(set(p.keys()).union(set(q.keys())))\n",
    "    p_vec = np.array([p.get(k, 0) for k in all_keys])\n",
    "    q_vec = np.array([q.get(k, 0) for k in all_keys])\n",
    "\n",
    "    return wasserstein_distance(p_vec, q_vec)\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    p = np.array(list(p.values())) + 1e-10\n",
    "    q = np.array(list(q.values())) + 1e-10\n",
    "    return stats.entropy(p, q)\n",
    "\n",
    "def js_divergence(p, q):\n",
    "    p = np.array(list(p.values())) + 1e-10\n",
    "    q = np.array(list(q.values())) + 1e-10\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * (stats.entropy(p, m) + stats.entropy(q, m))\n",
    "\n",
    "def entropy(p):\n",
    "    p = np.array(list(p.values())) + 1e-10\n",
    "    return stats.entropy(p)\n",
    "\n",
    "def rank_stability(dict_list):\n",
    "    ranks = [{k: rank for rank, (k, _) in enumerate(sorted(d.items(), key=lambda x: -x[1]))} for d in dict_list]\n",
    "    diffs = [sum(abs(ranks[i][k] - ranks[i-1].get(k, len(ranks[i]))) for k in ranks[i]) for i in range(1, len(ranks))]\n",
    "    return np.mean(diffs)\n",
    "\n",
    "def jaccard_similarity(p, q, k=5):\n",
    "    top_p = set(sorted(p, key=p.get, reverse=True)[:k])\n",
    "    top_q = set(sorted(q, key=q.get, reverse=True)[:k])\n",
    "    return len(top_p & top_q) / len(top_p | top_q)\n",
    "\n",
    "def most_common_pos(dict_list):\n",
    "    pos_counts = Counter()\n",
    "    for d in dict_list:\n",
    "        for word in d.keys():\n",
    "            pos_counts[get_pos(word)] += 1\n",
    "    return pos_counts.most_common(1)[0][0]\n",
    "\n",
    "def semantic_drift(first_dist, last_dist):\n",
    "    return 1 - cosine_similarity(first_dist, last_dist)  # Higher = more meaning change\n",
    "\n",
    "def word_surprise(first_dist, last_dist):\n",
    "    return kl_divergence(first_dist, last_dist)\n",
    "\n",
    "def top_k_jaccard(first_dist, last_dist, k=5):\n",
    "    return jaccard_similarity(first_dist, last_dist, k)\n",
    "\n",
    "def pos_stability(first_dict, last_dict):\n",
    "    first_pos = get_pos(get_first_guess([first_dict]))  # Get POS of most probable first guess\n",
    "    last_pos = get_pos(get_first_guess([last_dict]))  # Get POS of most probable last guess\n",
    "    return 1 if first_pos == last_pos else 0  # 1 = Same POS, 0 = Different POS\n",
    "\n",
    "def entropy_change(first_dist, last_dist):\n",
    "    return abs(entropy(first_dist) - entropy(last_dist))  # Absolute difference in uncertainty\n",
    "\n",
    "def compute_metrics(first_dict, last_dict):\n",
    "    if not first_dict or not last_dict:\n",
    "        return None  \n",
    "\n",
    "    metrics = {\n",
    "        'Cosine_Similarity': cosine_similarity(first_dict, last_dict),\n",
    "        'Wasserstein_Distance': wasserstein_similarity(first_dict, last_dict),\n",
    "        'Entropy_First': entropy(first_dict),\n",
    "        'Entropy_Last': entropy(last_dict),\n",
    "        'Entropy_Change': entropy_change(first_dict, last_dict),\n",
    "        'Rank_Stability': rank_stability([first_dict, last_dict]),\n",
    "        'Temporal_Jaccard': jaccard_similarity(first_dict, last_dict),\n",
    "        'Semantic_Drift': semantic_drift(first_dict, last_dict),\n",
    "        'Word_Surprise': word_surprise(first_dict, last_dict),\n",
    "        'Top_K_Jaccard': top_k_jaccard(first_dict, last_dict),\n",
    "        'POS_Stability': pos_stability(first_dict, last_dict),\n",
    "        'Most_Common_POS_First': get_pos(get_first_guess([first_dict])),\n",
    "        'Most_Common_POS_Last': get_pos(get_first_guess([last_dict])),\n",
    "        'First_Guess': get_first_guess([first_dict]),\n",
    "        'Last_Guess': get_first_guess([last_dict])\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_metrics(trial_metrics_list):\n",
    "    # Remove empty or None trials\n",
    "    trial_metrics_list = [trial for trial in trial_metrics_list if trial]\n",
    "\n",
    "    if not trial_metrics_list:  # If all trials were empty, return an empty dict\n",
    "        print(\"Warning: No valid trials found for aggregation.\")\n",
    "        return {}\n",
    "\n",
    "    aggregated = {}\n",
    "\n",
    "    # Updated numerical metrics (includes new metrics)\n",
    "    numerical_keys = [\n",
    "        'Cosine_Similarity', 'Wasserstein_Distance', 'Entropy_First', 'Entropy_Last', \n",
    "        'Entropy_Change', 'Rank_Stability', 'Temporal_Jaccard', \n",
    "        'Semantic_Drift', 'Word_Surprise', 'Top_K_Jaccard', 'POS_Stability'\n",
    "    ]\n",
    "\n",
    "    for key in numerical_keys:\n",
    "        try:\n",
    "            values = [trial[key] for trial in trial_metrics_list if key in trial and trial[key] is not None]\n",
    "\n",
    "            if values:  # Ensure non-empty list to avoid errors\n",
    "                mean_val = np.mean(values)\n",
    "                std_val = np.std(values, ddof=1) if len(values) > 1 else 0  # Avoid division by zero\n",
    "                ci_95 = (\n",
    "                    mean_val - 1.96 * (std_val / np.sqrt(len(values))) if len(values) > 1 else mean_val,\n",
    "                    mean_val + 1.96 * (std_val / np.sqrt(len(values))) if len(values) > 1 else mean_val\n",
    "                )\n",
    "\n",
    "                aggregated[key] = {\n",
    "                    'Mean': mean_val,\n",
    "                    'STD': std_val,\n",
    "                    'CI_95': ci_95\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {key} due to missing data: {e}\")\n",
    "\n",
    "    # Aggregate categorical variables (POS and First/Last Guess)\n",
    "    try:\n",
    "        pos_first_counts = Counter(trial['Most_Common_POS_First'] for trial in trial_metrics_list if 'Most_Common_POS_First' in trial and trial['Most_Common_POS_First'])\n",
    "        pos_last_counts = Counter(trial['Most_Common_POS_Last'] for trial in trial_metrics_list if 'Most_Common_POS_Last' in trial and trial['Most_Common_POS_Last'])\n",
    "\n",
    "        aggregated['Most_Common_POS_First'] = pos_first_counts.most_common(1)[0][0] if pos_first_counts else None\n",
    "        aggregated['Most_Common_POS_Last'] = pos_last_counts.most_common(1)[0][0] if pos_last_counts else None\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping POS aggregation due to missing data: {e}\")\n",
    "\n",
    "    try:\n",
    "        first_guess_counts = Counter(trial['First_Guess'] for trial in trial_metrics_list if 'First_Guess' in trial and trial['First_Guess'])\n",
    "        last_guess_counts = Counter(trial['Last_Guess'] for trial in trial_metrics_list if 'Last_Guess' in trial and trial['Last_Guess'])\n",
    "\n",
    "        aggregated['First_Guess'] = first_guess_counts.most_common(1)[0][0] if first_guess_counts else None\n",
    "        aggregated['Last_Guess'] = last_guess_counts.most_common(1)[0][0] if last_guess_counts else None\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping First/Last Guess aggregation due to missing data: {e}\")\n",
    "\n",
    "    return aggregated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word_trials(words, dataframe, n):\n",
    "    results = []\n",
    "    total_words = len(words)\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, word in enumerate(words, 1):\n",
    "        word_start_time = time.time()\n",
    "        print(f\"Processing word {idx}/{total_words}: {word}\")\n",
    "\n",
    "        # Step 1: Extract and filter data\n",
    "        try:\n",
    "            covid = filter_and_extract_word_stats(dataframe, word)\n",
    "            maskcovid = mask_df(covid, word)\n",
    "            the_df = get_random_samples(maskcovid, n)\n",
    "\n",
    "            if the_df.empty:\n",
    "                print(f\"Skipping {word}: No valid samples found.\")\n",
    "                continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {word}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Step 2: Compute meaning accumulation for first and last appearances\n",
    "        try:\n",
    "            first_appearance_dict_list = [accumulate_meaning_unrecognized(trial_df, 'first_appearance', word) for trial_df in the_df]\n",
    "            last_appearance_dict_list = [accumulate_meaning_unrecognized(trial_df, 'last_appearance', word) for trial_df in the_df]\n",
    "\n",
    "            # Remove any None or empty dictionaries from results\n",
    "            first_appearance_dict_list = [d for d in first_appearance_dict_list if d]\n",
    "            last_appearance_dict_list = [d for d in last_appearance_dict_list if d]\n",
    "\n",
    "            if not first_appearance_dict_list or not last_appearance_dict_list:\n",
    "                print(f\"Skipping {word}: No valid meaning accumulation results.\")\n",
    "                continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error accumulating meaning for {word}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Step 3: Compute Metrics\n",
    "        try:\n",
    "            first_metrics = [compute_metrics(first_dict, last_dict) for first_dict, last_dict in zip(first_appearance_dict_list, last_appearance_dict_list)]\n",
    "            aggregated_metrics = aggregate_metrics(first_metrics)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing metrics for {word}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Step 4: Store Aggregated Results\n",
    "        row = {'Word': word}\n",
    "        for metric, values in aggregated_metrics.items():\n",
    "            if isinstance(values, dict):\n",
    "                row.update({\n",
    "                    f'{metric}_Mean': values['Mean'],\n",
    "                    f'{metric}_STD': values['STD'],\n",
    "                    f'{metric}_CI': values['CI_95']\n",
    "                })\n",
    "            else:\n",
    "                row[metric] = values\n",
    "\n",
    "        results.append(row)\n",
    "\n",
    "        # Step 5: Time tracking and progress update\n",
    "        elapsed_time = time.time() - word_start_time\n",
    "        remaining_time = ((time.time() - start_time) / idx) * (total_words - idx)\n",
    "        print(f\"Completed {word} in {elapsed_time:.2f} seconds. Estimated time remaining: {remaining_time:.2f} seconds.\")\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mlen\u001b[39m(energy_drink) \u001b[38;5;66;03m#1077\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# get words with at least 100 threads where their first and last appearance are different \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m valid_words_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcount_valid_threads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkarina\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menergy_drink\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# {word: valid_thread_count}\u001b[39;00m\n\u001b[1;32m      4\u001b[0m valid_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(valid_words_dict\u001b[38;5;241m.\u001b[39mkeys())\n",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m, in \u001b[0;36mcount_valid_threads\u001b[0;34m(thread_dict, words)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m thread_id, comments \u001b[38;5;129;01min\u001b[39;00m thread_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Find all comments containing the target word\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     appearances \u001b[38;5;241m=\u001b[39m (c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m comments \u001b[38;5;28;01mif\u001b[39;00m word_pattern\u001b[38;5;241m.\u001b[39msearch(c[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomment\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m---> 13\u001b[0m     appearances_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(appearances)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Count only threads where the word appears at least twice\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(appearances_list) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m valid_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m thread_id, comments \u001b[38;5;129;01min\u001b[39;00m thread_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Find all comments containing the target word\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     appearances \u001b[38;5;241m=\u001b[39m (c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m comments \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mword_pattern\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcomment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     13\u001b[0m     appearances_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(appearances)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Count only threads where the word appears at least twice\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "len(energy_drink) #1077\n",
    "# get words with at least 100 threads where their first and last appearance are different \n",
    "valid_words_dict = count_valid_threads(karina, energy_drink)  # {word: valid_thread_count}\n",
    "valid_words = list(valid_words_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(energy_drink) #1077\n",
    "sublists = [energy_drink[i:i + 100] for i in range(0, len(energy_drink), 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_sublists(the_sublists, dataframe, n_trials, folder_name=\"first_round\"):\n",
    "    os.makedirs(folder_name, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "    \n",
    "    for idx, sublist in enumerate(the_sublists, 1):\n",
    "        print(f\"Processing sublist {idx}/10...\")\n",
    "        df = process_word_trials(sublist, dataframe, n_trials)\n",
    "        file_path = os.path.join(folder_name, f\"first_round_sublist_{idx}.csv\")\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"Saved {file_path}\")\n",
    "    \n",
    "    print(\"All sublists processed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sublist 1/10...\n",
      "Processing word 1/100: khazaria\n",
      "Completed khazaria in 12.24 seconds. Estimated time remaining: 1211.52 seconds.\n",
      "Processing word 2/100: christkike\n",
      "Completed christkike in 11.78 seconds. Estimated time remaining: 1176.84 seconds.\n",
      "Processing word 3/100: soiboi\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#started feb 18 10:28PM \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mprocess_and_save_sublists\u001b[49m\u001b[43m(\u001b[49m\u001b[43msublists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkarina\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[112], line 6\u001b[0m, in \u001b[0;36mprocess_and_save_sublists\u001b[0;34m(the_sublists, dataframe, n_trials, folder_name)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, sublist \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(the_sublists, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing sublist \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/10...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_word_trials\u001b[49m\u001b[43m(\u001b[49m\u001b[43msublist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_name, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst_round_sublist_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_csv(file_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[104], line 12\u001b[0m, in \u001b[0;36mprocess_word_trials\u001b[0;34m(words, dataframe, n)\u001b[0m\n\u001b[1;32m     10\u001b[0m covid \u001b[38;5;241m=\u001b[39m filter_and_extract_word_stats(dataframe, word)\n\u001b[1;32m     11\u001b[0m maskcovid \u001b[38;5;241m=\u001b[39m mask_df(covid, word)\n\u001b[0;32m---> 12\u001b[0m the_df \u001b[38;5;241m=\u001b[39m \u001b[43mget_random_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaskcovid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m first_appearance_dict_list \u001b[38;5;241m=\u001b[39m [accumulate_meaning_unrecognized(trial_df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst_appearance\u001b[39m\u001b[38;5;124m'\u001b[39m, word) \u001b[38;5;28;01mfor\u001b[39;00m trial_df \u001b[38;5;129;01min\u001b[39;00m the_df]\n\u001b[1;32m     15\u001b[0m last_appearance_dict_list \u001b[38;5;241m=\u001b[39m [accumulate_meaning_unrecognized(trial_df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_appearance\u001b[39m\u001b[38;5;124m'\u001b[39m, word) \u001b[38;5;28;01mfor\u001b[39;00m trial_df \u001b[38;5;129;01min\u001b[39;00m the_df]\n",
      "Cell \u001b[0;32mIn[76], line 4\u001b[0m, in \u001b[0;36mget_random_samples\u001b[0;34m(df, trial_num)\u001b[0m\n\u001b[1;32m      2\u001b[0m dfs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, trial_num \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dfs\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/pandas/core/generic.py:6118\u001b[0m, in \u001b[0;36mNDFrame.sample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[0m\n\u001b[1;32m   6115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   6116\u001b[0m     weights \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mpreprocess_weights(\u001b[38;5;28mself\u001b[39m, weights, axis)\n\u001b[0;32m-> 6118\u001b[0m sampled_indices \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6119\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(sampled_indices, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   6121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/pandas/core/sample.py:152\u001b[0m, in \u001b[0;36msample\u001b[0;34m(obj_len, size, replace, weights, random_state)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weights: weights sum to zero\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\n\u001b[1;32m    153\u001b[0m     np\u001b[38;5;241m.\u001b[39mintp, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    154\u001b[0m )\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:1024\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "#started feb 18 10:28PM \n",
    "process_and_save_sublists(sublists, karina, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing word 1/3: covid\n",
      "Completed covid in 13.72 seconds. Estimated time remaining: 27.44 seconds.\n",
      "Processing word 2/3: russia\n",
      "Completed russia in 17.07 seconds. Estimated time remaining: 15.39 seconds.\n",
      "Processing word 3/3: china\n",
      "Completed china in 15.45 seconds. Estimated time remaining: 0.00 seconds.\n",
      "         51607975 function calls (50628379 primitive calls) in 46.233 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 1055 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.005    0.005   46.236   46.236 /var/folders/87/9qwg67f164gck_jk0qq9fkc40000gn/T/ipykernel_37158/2236755408.py:1(process_word_trials)\n",
      "       30    0.013    0.000   19.351    0.645 /var/folders/87/9qwg67f164gck_jk0qq9fkc40000gn/T/ipykernel_37158/3099757713.py:1(accumulate_meaning_unrecognized)\n",
      "       30    0.000    0.000   19.223    0.641 /Users/easy/.pyenv/versions/3.11.5/lib/python3.11/site-packages/transformers/pipelines/fill_mask.py:248(__call__)\n",
      "       30    0.075    0.003   19.222    0.641 /Users/easy/.pyenv/versions/3.11.5/lib/python3.11/site-packages/transformers/pipelines/base.py:1240(__call__)\n",
      " 1260/630    0.071    0.000   19.145    0.030 /Users/easy/.pyenv/versions/3.11.5/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:117(__next__)\n",
      "149670/147210    0.035    0.000   18.744    0.000 {built-in method builtins.next}\n",
      "      600    0.007    0.000   18.449    0.031 /Users/easy/.pyenv/versions/3.11.5/lib/python3.11/site-packages/transformers/pipelines/base.py:1200(forward)\n",
      "      600    0.003    0.000   15.586    0.026 /Users/easy/.pyenv/versions/3.11.5/lib/python3.11/site-packages/transformers/pipelines/fill_mask.py:126(_forward)\n",
      "253200/600    0.123    0.000   15.581    0.026 /Users/easy/.pyenv/versions/3.11.5/lib/python3.11/site-packages/torch/nn/modules/module.py:1732(_wrapped_call_impl)\n",
      "253200/600    0.258    0.000   15.580    0.026 /Users/easy/.pyenv/versions/3.11.5/lib/python3.11/site-packages/torch/nn/modules/module.py:1740(_call_impl)\n",
      "      600    0.004    0.000   15.578    0.026 /Users/easy/.pyenv/versions/3.11.5/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:1170(forward)\n",
      "      600    0.014    0.000   15.299    0.025 /Users/easy/.pyenv/versions/3.11.5/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:835(forward)\n",
      "      600    0.021    0.000   12.388    0.021 /Users/easy/.pyenv/versions/3.11.5/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:587(forward)\n",
      "    14400    0.033    0.000   12.315    0.001 /Users/easy/.pyenv/versions/3.11.5/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:508(forward)\n",
      "        3    0.000    0.000   11.508    3.836 /var/folders/87/9qwg67f164gck_jk0qq9fkc40000gn/T/ipykernel_37158/4023240230.py:1(mask_df)\n",
      "       12    0.000    0.000   11.474    0.956 /Users/easy/.pyenv/versions/3.11.5/lib/python3.11/site-packages/pandas/core/series.py:4789(apply)\n",
      "       12    0.000    0.000   11.474    0.956 /Users/easy/.pyenv/versions/3.11.5/lib/python3.11/site-packages/pandas/core/apply.py:1409(apply)\n",
      "       12    0.000    0.000   11.474    0.956 /Users/easy/.pyenv/versions/3.11.5/lib/python3.11/site-packages/pandas/core/apply.py:1482(apply_standard)\n",
      "       12    0.000    0.000   11.472    0.956 /Users/easy/.pyenv/versions/3.11.5/lib/python3.11/site-packages/pandas/core/base.py:891(_map_values)\n",
      "       12    0.069    0.006   11.472    0.956 /Users/easy/.pyenv/versions/3.11.5/lib/python3.11/site-packages/pandas/core/algorithms.py:1667(map_array)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#YOU'RE IN THE DEBUGGING SECTION NOW -----\n",
    "covid = filter_and_extract_word_stats(karina, \"covid\") # Get a dataframe of the input word, and its first and last appearances\n",
    "maskcovid = mask_df(covid, 'covid') # Mask the input word\n",
    "the_df = get_random_samples(maskcovid, 5)\n",
    "first_appearance_dict_list = [accumulate_meaning_unrecognized(trial_df, 'first_appearance', \"covid\") for trial_df in the_df] # Get the probability distributions\n",
    "n = \"whatever trial number we're on\"\n",
    "word = \"some word\"\n",
    "covid = filter_and_extract_word_stats(karina, \"covid\") # Get a dataframe of the input word, and its first and last appearances\n",
    "maskcovid = mask_df(covid, 'covid') # Mask the input word\n",
    "the_df = get_random_samples(maskcovid, 5)\n",
    "first_appearance_dict_list = accumulate_meaning_unrecognized(the_df, 'first_appearance', \"covid\") # Get the probability distributions\n",
    "last_appearance_dict_list = accumulate_meaning_unrecognized(the_df, 'last_appearance', \"some word\")\n",
    "first = compute_metrics(first_appearance_dict_list) # Get the metrics for each trial \n",
    "last = compute_metrics(last_appearance_dict_list)\n",
    "first = aggregate_metrics(first)\n",
    "last = aggregate_metrics(last)\n",
    "#add this to a dataframe where each row is the input word (assume we're passing in a list of words)\n",
    "#each column in the dataframe is each first/last appearance's respective metrics, and respective mean/STD socres \n",
    "\n",
    "test_list = ['covid', 'russia', 'china']\n",
    "\n",
    "def profile_code():\n",
    "    words = [\"covid\", \"russia\", \"china\"]  # Test with a few words first\n",
    "    dataframe = karina  # Your input DataFrame\n",
    "    n = 5  # Number of trials per word\n",
    "\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "    process_word_trials(words, dataframe, n)  # Replace with your function call\n",
    "    profiler.disable()\n",
    "\n",
    "    s = io.StringIO()\n",
    "    sortby = 'cumulative'  # Sort by total time spent in each function\n",
    "    ps = pstats.Stats(profiler, stream=s).sort_stats(sortby)\n",
    "    ps.print_stats(20)  # Show top 20 slowest functions\n",
    "    print(s.getvalue())\n",
    "\n",
    "# Run the profiling\n",
    "profile_code()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
