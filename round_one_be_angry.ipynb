{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/easy/.pyenv/versions/3.11.5/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time \n",
    "import random\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import entropy, spearmanr\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.spatial.distance import jensenshannon, cosine\n",
    "import regex as re\n",
    "import spacy\n",
    "import string\n",
    "import os\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "import json\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "device_id = 0 if torch.backends.mps.is_available() else -1\n",
    "dm = pd.read_csv(\"gecko_pelts.csv\", index_col = 0)\n",
    "energy_drink = [x for x in dm['word']]\n",
    "da = pd.read_csv(\"jan_mar_2023poldumps.csv\", index_col = 0)\n",
    "dg = da.dropna().drop_duplicates(subset=\"comment\")\n",
    "model_name = \"roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name).to(device)\n",
    "fill_mask_pipeline = pipeline(\"fill-mask\", model=model_name, device=device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ninety = pd.read_csv(\"unrecognized_words_90cap.csv\", index_col = 0)\n",
    "ninetylist = [x for x in ninety['word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1077"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(energy_drink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1782"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ninetylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adds = [x for x in ninetylist if x not in energy_drink]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "727"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_prose(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r\"^>\\s*\", \"\", text)  # Remove leading '>'\n",
    "    text = re.sub(r\">>\\d+\\s*\", \"\", text)  # Remove quote references\n",
    "    text = re.sub(r\"^[^a-zA-Z]+|[^a-zA-Z.!?]+$\", \"\", text)  # Trim unwanted characters\n",
    "    text = re.sub(r\"\\n+\", \" \", text)  # Replace newlines with spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize whitespace\n",
    "    text = re.sub(r\"http[s]?://[^\\s>]+|www\\.[^\\s>]+\", \"\", text)  # Remove URLs\n",
    "    return text\n",
    "\n",
    "def split_sentences(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    return [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n",
    "\n",
    "def filter_by_word(df, column, word):\n",
    "    if not isinstance(word, str):\n",
    "        raise ValueError(f\"Invalid word input: expected string, got {type(word)}\")\n",
    "    \n",
    "    pattern = re.compile(rf'\\b{re.escape(word)}\\b', re.IGNORECASE)\n",
    "    return df[df[column].astype(str).str.contains(pattern, na=False)]\n",
    "\n",
    "def filter_strings_by_word(sentences, word):\n",
    "    if not isinstance(word, str):\n",
    "        raise ValueError(f\"Invalid word input: expected string, got {type(word)}\")\n",
    "    \n",
    "    pattern = re.compile(rf'\\b{re.escape(word)}\\b', re.IGNORECASE)\n",
    "    return [s for s in sentences if pattern.search(s)]\n",
    "\n",
    "def is_valid_word(word):\n",
    "    return bool(re.match(r\"^[a-zA-Z]+$\", word))\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    df.drop_duplicates(subset=\"comment\", keep=\"first\", inplace=True)\n",
    "    df = df[df[\"comment\"].apply(lambda x: len(str(x).split(\" \")) <= 512)]\n",
    "    return df.reset_index(drop=True) if len(df) >= 100 else pd.DataFrame()\n",
    "\n",
    "def og_thedf(df, word):\n",
    "    df[\"comment\"] = df[\"comment\"].apply(clean_prose)\n",
    "    df = filter_by_word(df, \"comment\", word)\n",
    "    df[\"com_list_it1\"] = df[\"comment\"].apply(split_sentences)\n",
    "    df[\"sent_with_word\"] = df[\"com_list_it1\"].apply(lambda x: filter_strings_by_word(x, word))\n",
    "    df = df.explode(\"sent_with_word\").drop_duplicates(subset=[\"comment\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def getopdf(df, word):\n",
    "    return og_thedf(df, word)[lambda x: x[\"op\"] == 1]\n",
    "\n",
    "def group_threads_by_timestamp(df, thread_col=\"thread_num\", num_col=\"num\", comment_col=\"comment\", timestamp_col=\"timestamp\"):\n",
    "    thread_dict = defaultdict(list)\n",
    "    for thread, thread_df in df.groupby(thread_col):\n",
    "        sorted_comments = thread_df.sort_values(by=timestamp_col)[[num_col, comment_col]].to_dict(orient='records')\n",
    "        thread_dict[thread] = sorted_comments\n",
    "    return thread_dict\n",
    "\n",
    "def df_to_thread_dict(df):\n",
    "    thread_dict = {\n",
    "        thread: thread_df.sort_values(by=\"timestamp\")[[\"num\", \"comment\"]].to_dict(orient='records')\n",
    "        for thread, thread_df in df.groupby(\"thread_num\")\n",
    "    }\n",
    "    return thread_dict\n",
    "\n",
    "def precompute_thread_dict(df):\n",
    "    thread_dict = {}\n",
    "    for thread, thread_df in df.groupby(\"thread_num\"):\n",
    "        thread_dict[thread] = thread_df.sort_values(by=\"timestamp\")[[\"num\", \"comment\"]].to_dict(orient='records')\n",
    "    return thread_dict\n",
    "\n",
    "def pretokenize_comments(thread_dict):\n",
    "    for thread, posts in thread_dict.items():\n",
    "        for post in posts:\n",
    "            post['tokenized_comment'] = set(post['comment'].lower().split())  # Store pre-tokenized words\n",
    "    return thread_dict\n",
    "\n",
    "karina = pretokenize_comments(precompute_thread_dict(dg)) ### TURNING DATAFRAME INTO A DICTIONARY OF THREADS!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_extract_word_stats(thread_dict, target_word):\n",
    "    if not isinstance(target_word, str):\n",
    "        raise ValueError(f\"Invalid target_word input: expected string, got {type(target_word)}\")\n",
    "    \n",
    "    word_pattern = re.compile(rf'\\b{re.escape(target_word)}\\b', re.IGNORECASE)\n",
    "    data = []\n",
    "    \n",
    "    for thread_id, comments in thread_dict.items():\n",
    "        appearances = (c for c in comments if word_pattern.search(c['comment']))\n",
    "        appearances_list = list(appearances)\n",
    "        if appearances_list:\n",
    "            first_comment = appearances_list[0]\n",
    "            \n",
    "            data.append({\n",
    "                'thread_id': thread_id,\n",
    "                'first_appearance': first_comment['comment'],\n",
    "                'first_id': first_comment['num'],\n",
    "                'length': len(comments)  # Total number of comments in thread\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def mask_df(a_df, a_word, tokenizer=tokenizer):\n",
    "    if not isinstance(a_word, str):\n",
    "        raise ValueError(f\"Invalid a_word input: expected string, got {type(a_word)}\")\n",
    "    \n",
    "    mask_token = tokenizer.mask_token\n",
    "    word_pattern = re.compile(rf'\\b{re.escape(a_word)}\\b', re.IGNORECASE)\n",
    "    \n",
    "    def masker(text):\n",
    "        return word_pattern.sub(mask_token, text) if isinstance(text, str) else text\n",
    "    \n",
    "    def trim_long_text(text, limit=450):\n",
    "        if not isinstance(text, str):\n",
    "            return text\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        if len(tokens) <= limit:\n",
    "            return text\n",
    "        words = text.split()\n",
    "        word_indices = [i for i, w in enumerate(words) if word_pattern.search(w)]\n",
    "        if not word_indices:\n",
    "            return \" \".join(words[:limit])\n",
    "        \n",
    "        target_index = word_indices[0]\n",
    "        left, right = max(0, target_index - limit // 2), min(len(words), target_index + limit // 2)\n",
    "        return \" \".join(words[left:right])\n",
    "    \n",
    "    # Apply Masking and Filtering\n",
    "    a_df[\"first_appearance\"] = a_df[\"first_appearance\"].apply(masker)\n",
    "    a_df = a_df[a_df[\"first_appearance\"].str.contains(mask_token, na=False)]\n",
    "    a_df[\"first_appearance\"] = a_df[\"first_appearance\"].apply(trim_long_text)\n",
    "    \n",
    "    return a_df\n",
    "\n",
    "\n",
    "def get_random_samples(df, trial_num):\n",
    "    dfs = []\n",
    "    for i in range(1, trial_num + 1):\n",
    "        dfs.append(df.sample(n=20, random_state=i))\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_meaning_unrecognized(df, mask_column, target_word, fill_mask_pipeline=fill_mask_pipeline, tokenizer=tokenizer, top_k=10):\n",
    "    predictions_list = []\n",
    "    mask_token = tokenizer.mask_token\n",
    "    word_pattern = re.compile(rf'\\b{re.escape(target_word)}\\b', re.IGNORECASE)\n",
    "    punctuation_set = set(string.punctuation)\n",
    "    df[mask_column] = df[mask_column].astype(str)\n",
    "\n",
    "    sentences = df[mask_column].str.strip().tolist()\n",
    "    processed_sentences = [word_pattern.sub(mask_token, sentence.lower()) for sentence in sentences if sentence]\n",
    "\n",
    "    # Skip sentences longer than 512 tokens and print a warning\n",
    "    filtered_sentences = []\n",
    "    for sentence in processed_sentences:\n",
    "        if len(tokenizer.tokenize(sentence)) > 512:\n",
    "            print(f\"Skipping sentence (exceeds 512 tokens): {sentence[:100]}...\")\n",
    "            continue\n",
    "        filtered_sentences.append(sentence)\n",
    "    \n",
    "    if not filtered_sentences:\n",
    "        return predictions_list  # Return empty if no valid sentences remain\n",
    "\n",
    "    # Batch tokenization with GPU support\n",
    "    tokenized_sentences = tokenizer.batch_encode_plus(filtered_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    tokenized_sentences = {k: v.to(device) for k, v in tokenized_sentences.items()}  # Move to GPU if available\n",
    "\n",
    "    # **Batch Inference Optimization with torch.no_grad()**\n",
    "    with torch.no_grad():\n",
    "        predictions_batch = fill_mask_pipeline(filtered_sentences)  # **Send all at once instead of looping**\n",
    "    \n",
    "    for predictions in predictions_batch:\n",
    "        if not isinstance(predictions, list) or 'token_str' not in predictions[0]:\n",
    "            continue\n",
    "\n",
    "        filtered_predictions = {\n",
    "            p['token_str'].strip(): np.log(p['score'])\n",
    "            for p in predictions\n",
    "            if 'token_str' in p and not all(char in punctuation_set for char in p['token_str'])\n",
    "        }\n",
    "\n",
    "        if not filtered_predictions:\n",
    "            continue\n",
    "\n",
    "        log_probs = np.array(list(filtered_predictions.values()))\n",
    "        max_log_prob = np.max(log_probs)\n",
    "        exp_probs = np.exp(log_probs - max_log_prob)\n",
    "        normalized_probs = dict(zip(filtered_predictions.keys(), exp_probs / exp_probs.sum()))\n",
    "\n",
    "        predictions_list.append(normalized_probs)\n",
    "\n",
    "    return predictions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(word):\n",
    "    doc = nlp(word)\n",
    "    return doc[0].pos_ if doc and doc[0].pos_ else \"UNKNOWN\"\n",
    "\n",
    "def get_first_guess(dict_list):\n",
    "    guess_counts = Counter()\n",
    "    for d in dict_list:\n",
    "        if d:\n",
    "            best_guess = max(d, key=d.get)  # Get the word with the highest probability\n",
    "            guess_counts[best_guess] += 1\n",
    "    return guess_counts.most_common(1)[0][0] if guess_counts else None\n",
    "\n",
    "def cosine_similarity(p, q):\n",
    "    all_keys = set(p.keys()).union(set(q.keys()))\n",
    "    p_vec = np.array([p.get(k, 0) for k in all_keys])\n",
    "    q_vec = np.array([q.get(k, 0) for k in all_keys])\n",
    "    \n",
    "    if np.all(p_vec == 0) or np.all(q_vec == 0):  # Avoid division by zero\n",
    "        return 0.0\n",
    "    return 1 - cosine(p_vec, q_vec)\n",
    "\n",
    "def wasserstein_similarity(p, q):\n",
    "    all_keys = list(set(p.keys()).union(set(q.keys())))\n",
    "    p_vec = np.array([p.get(k, 0) for k in all_keys])\n",
    "    q_vec = np.array([q.get(k, 0) for k in all_keys])\n",
    "\n",
    "    return wasserstein_distance(p_vec, q_vec)\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    p = np.array(list(p.values())) + 1e-10\n",
    "    q = np.array(list(q.values())) + 1e-10\n",
    "    return stats.entropy(p, q)\n",
    "\n",
    "def js_divergence(p, q):\n",
    "    p = np.array(list(p.values())) + 1e-10\n",
    "    q = np.array(list(q.values())) + 1e-10\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * (stats.entropy(p, m) + stats.entropy(q, m))\n",
    "\n",
    "def entropy(p):\n",
    "    p = np.array(list(p.values())) + 1e-10\n",
    "    return stats.entropy(p)\n",
    "\n",
    "def rank_stability(dict_list):\n",
    "    ranks = [{k: rank for rank, (k, _) in enumerate(sorted(d.items(), key=lambda x: -x[1]))} for d in dict_list]\n",
    "    diffs = [sum(abs(ranks[i][k] - ranks[i-1].get(k, len(ranks[i]))) for k in ranks[i]) for i in range(1, len(ranks))]\n",
    "    return np.mean(diffs)\n",
    "\n",
    "def jaccard_similarity(p, q, k=5):\n",
    "    top_p = set(sorted(p, key=p.get, reverse=True)[:k])\n",
    "    top_q = set(sorted(q, key=q.get, reverse=True)[:k])\n",
    "    return len(top_p & top_q) / len(top_p | top_q)\n",
    "\n",
    "def most_common_pos(dict_list):\n",
    "    pos_counts = Counter()\n",
    "    for d in dict_list:\n",
    "        for word in d.keys():\n",
    "            pos_counts[get_pos(word)] += 1\n",
    "    return pos_counts.most_common(1)[0][0]\n",
    "\n",
    "def compute_metrics(dict_list):\n",
    "    if not dict_list or len(dict_list) < 2:\n",
    "        return {} \n",
    "\n",
    "    metrics = {\n",
    "        'Cosine_Similarity': np.mean([cosine_similarity(dict_list[i], dict_list[i-1]) for i in range(1, len(dict_list))]),\n",
    "        'Wasserstein_Distance': np.mean([wasserstein_similarity(dict_list[i], dict_list[i-1]) for i in range(1, len(dict_list))]),\n",
    "        'Entropy' : np.mean([entropy(dict_list[i]) for i in range(1, len(dict_list))]),\n",
    "        'Entropy_Diff': np.mean([entropy(dict_list[i]) - entropy(dict_list[i-1]) for i in range(1, len(dict_list))]),\n",
    "        'Rank_Stability': rank_stability(dict_list),\n",
    "        'Temporal_Jaccard': np.mean([jaccard_similarity(dict_list[i], dict_list[i-1]) for i in range(1, len(dict_list))]),\n",
    "        'Most_Common_POS': most_common_pos(dict_list),\n",
    "        'First_Guess': get_first_guess(dict_list)\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_metrics(trial_metrics_list):\n",
    "    trial_metrics_list = [trial for trial in trial_metrics_list if trial]\n",
    "\n",
    "    if not trial_metrics_list:  # If all trials were empty, return an empty dict\n",
    "        print(\"Warning: No valid trials found for aggregation.\")\n",
    "        return {}\n",
    "\n",
    "    aggregated = {}\n",
    "    numerical_keys = ['Cosine_Similarity', 'Wasserstein_Distance', 'Entropy', 'Entropy_Diff', 'Rank_Stability', 'Temporal_Jaccard']\n",
    "    \n",
    "    for key in numerical_keys:\n",
    "        try:\n",
    "            values = [trial[key] for trial in trial_metrics_list if key in trial] \n",
    "            if values:\n",
    "                aggregated[key] = {\n",
    "                    'Mean': np.mean(values),\n",
    "                    'STD': np.std(values, ddof=1),\n",
    "                    'CI_95': (np.mean(values) - 1.96 * (np.std(values, ddof=1) / np.sqrt(len(values))),\n",
    "                              np.mean(values) + 1.96 * (np.std(values, ddof=1) / np.sqrt(len(values))))\n",
    "                }\n",
    "        except KeyError as e:\n",
    "            print(f\"Skipping {key} due to missing data: {e}\")\n",
    "\n",
    "    pos_counts = Counter(trial['Most_Common_POS'] for trial in trial_metrics_list if 'Most_Common_POS' in trial)\n",
    "    aggregated['Most_Common_POS'] = pos_counts.most_common(1)[0][0] if pos_counts else None\n",
    "\n",
    "    first_guess_counts = Counter(trial['First_Guess'] for trial in trial_metrics_list if 'First_Guess' in trial)\n",
    "    aggregated['First_Guess_Guess'] = first_guess_counts.most_common(1)[0][0] if first_guess_counts else None\n",
    "\n",
    "    return aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word_trials(words, dataframe, n):\n",
    "    results = []\n",
    "    total_words = len(words)\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, word in enumerate(words, 1):\n",
    "        word_start_time = time.time()\n",
    "        print(f\"Processing word {idx}/{total_words}: {word}\")\n",
    "\n",
    "        covid = filter_and_extract_word_stats(dataframe, word)\n",
    "        maskcovid = mask_df(covid, word)\n",
    "        the_df = get_random_samples(maskcovid, n)\n",
    "\n",
    "        first_appearance_dict_list = [accumulate_meaning_unrecognized(trial_df, 'first_appearance', word) for trial_df in the_df]\n",
    "\n",
    "        # Drop word if there's not enough data\n",
    "        if not any(first_appearance_dict_list) or all(len(trial) < 2 for trial in first_appearance_dict_list):\n",
    "            print(f\"Skipping word '{word}' due to insufficient data.\")\n",
    "            continue  # skip \n",
    "\n",
    "        first_metrics = [compute_metrics(trial) for trial in first_appearance_dict_list if trial]  \n",
    "        first_aggregated = aggregate_metrics(first_metrics)\n",
    "\n",
    "        row = {'Word': word}\n",
    "        for metric, values in first_aggregated.items():\n",
    "            if isinstance(values, dict):\n",
    "                row.update({f'First_{metric}_Mean': values['Mean'],\n",
    "                            f'First_{metric}_STD': values['STD'],\n",
    "                            f'First_{metric}_CI': values['CI_95']})\n",
    "            else:\n",
    "                row[f'First_{metric}'] = values\n",
    "\n",
    "        results.append(row)\n",
    "        elapsed_time = time.time() - word_start_time\n",
    "        remaining_time = (time.time() - start_time) / idx * (total_words - idx)\n",
    "        print(f\"Completed {word} in {elapsed_time:.2f} seconds. Estimated time remaining: {remaining_time:.2f} seconds.\")\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(energy_drink) #1077\n",
    "sublists = [adds[i:i + 100] for i in range(0, len(adds), 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_sublists(the_sublists, dataframe, n_trials, folder_name=\"first_round\"):\n",
    "    os.makedirs(folder_name, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "    \n",
    "    for idx, sublist in enumerate(the_sublists, 1):\n",
    "        print(f\"Processing sublist {idx}/10...\")\n",
    "        df = process_word_trials(sublist, dataframe, n_trials)\n",
    "        file_path = os.path.join(folder_name, f\"first_roundv2_sublist_{idx}.csv\")\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"Saved {file_path}\")\n",
    "    \n",
    "    print(\"All sublists processed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_save_sublists(sublists, karina, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#started feb 18 10:28PM \n",
    "#process_and_save_sublists(sublists, karina, 5)\n",
    "# error thrown at 11:18PM on sublist 2, NaN value in compute_metrics, changed to return empty dict instead of none\n",
    "# error thrown at 11:31 PM on sublist 2\n",
    "# error thrown at 11:41PM on sublist 2\n",
    "#sublists2 = sublists[1:]\n",
    "#process_and_save_sublists(sublists2, karina, 5)\n",
    "# rerun on feb 19 10:11AM to recapture 'first_guess' \n",
    "# added 'entropy' metric along with 'entropy_diff' \n",
    "# feb 19 6PM => testing changes if we change the cap to 90 threads instead of 176"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
